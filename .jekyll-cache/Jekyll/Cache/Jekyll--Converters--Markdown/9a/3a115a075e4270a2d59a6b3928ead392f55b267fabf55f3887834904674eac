I"o"<p><strong>Work in progress</strong></p>

<h1 id="paper">Paper</h1>
<hr />

<h2 id="sam-sharpness-aware-minimization-for-efficiently-improvinggeneralization">SAM: Sharpness-Aware Minimization for Efficiently ImprovingGeneralization</h2>
<div float="left" width="10%">
<p>
	<a href="https://arxiv.org/pdf/2010.01412v2.pdf">
		<img src="http://img.shields.io/badge/paper-arXiv.2010.01412v2-B3181B.svg" />
	</a>
</p>
</div>

<hr />

<h2 id="delf">DELF:</h2>
<p><a href="https://arxiv.org/abs/1612.06321">Large-Scale Image Retrieval with Attentive Deep Local Features</a>
   Image similarity based on learned local feature extraction</p>

<hr />
<p><a href="https://arxiv.org/abs/1612.06321" style="float: left"><img src="http://img.shields.io/badge/paper-arXiv.1612.06321-B3181B.svg" alt="Paper" /></a></p>

<p>DELG:
<a href="https://arxiv.org/abs/2001.05027"><img src="http://img.shields.io/badge/paper-arXiv.2001.05027-B3181B.svg" alt="Paper" /></a></p>

<h1 id="gem">GeM</h1>
<p><a href="https://arxiv.org/pdf/1711.02512"><img src="http://img.shields.io/badge/paper-arXiv.1711.02512-B3181B.svg" alt="Paper" /></a></p>

<p>Generalized-mean (GeM) pooling.</p>

<h1 id="mixed-pooling">Mixed Pooling</h1>
<p><a href="https://arxiv.org/pdf/1509.06033"><img src="http://img.shields.io/badge/paper-arXiv.1509.06033-B3181B.svg" alt="Paper" /></a></p>

<h1 id="comparison-of-mean-max-and-hybridboth-pooling">Comparison of mean, max and hybrid(both) pooling</h1>
<p><a href="https://arxiv.org/pdf/1509.06033.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1509.06033-B3181B.svg" alt="Paper" /></a>
Illustration of semantic information captured by each feature map of pool5 layer using CNN
-&gt; Mean pooling is better then just concat mean and max (hybrid)</p>

<h1 id="retrieving-similar-e-commerce-images-using-deep-learning">Retrieving Similar E-Commerce Images Using Deep Learning</h1>

<ul>
  <li>
    <p>Siamese architecture (2 images?), angular loss, combination of low and toplevel,
fractional distance matrix to calc distance between features</p>
  </li>
  <li>
    <p>Manhatten distance matrix provides the best discrimination in high dimensional data spaces.
L_k norm with k smaller then 1</p>
  </li>
</ul>

<h1 id="combination-of-multiple-global-descriptors-for-image-retrieval">Combination of Multiple Global Descriptors for Image Retrieval</h1>
<p><a href="https://arxiv.org/pdf/1903.10663.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1903.10663-B3181B.svg" alt="Paper" /></a></p>

<p>Instead of using multiple models for image retrival, which has shown to improve performance, this
paper aims to get multiple outputs of a single model and combine them to have a similar ensemble effect.
Basically they combine SPoC, Mac and GeM as pooling methods before the final layers in the network.
Rember SPoC deals with larger regions, Mac focuses on smaller details in the image.</p>

<p>Additionally used is label smoothing and temperature scaling. What is this?</p>

<p>“The temperature scaling with low-temperature parameterτin the Equation 4,  assigns a larger gradient
 to more chal-lenging  examples  and  is  helpful  for  intra-class  compact,and inter-class spread-out embedding.”
What does that mean?</p>

<p>“The label smooth-ing enhances a model, thereby improves generalization by
estimating the marginalized effect of a label-dropout duringtraining. Therefore, to prevent over-fitting,
 and learn betterembedding, we add label smoothing and temperature scal-ing in the auxiliary classification loss”</p>

<h1 id="graph-neural-networksa-review-of-methods-and-applications">Graph Neural Networks:A Review of Methods and Applications</h1>
<p><a href="https://arxiv.org/pdf/1812.08434.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1812.08434-B3181B.svg" alt="Paper" /></a></p>

<h1 id="self-correction-for-human-parsing">Self-Correction for Human Parsing</h1>
<p><a href="https://arxiv.org/pdf/1910.09777v1.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1910.09777-B3181B.svg" alt="Paper" /></a></p>

<h1 id="yolov4-optimal-speed-and-accuracy-of-object-detection">YOLOv4: Optimal Speed and Accuracy of Object Detection</h1>
<p><a href="https://arxiv.org/pdf/2004.10934v1.pdf"><img src="http://img.shields.io/badge/paper-arXiv.2004.10934v1-B3181B.svg" alt="Paper" /></a></p>

<p>Yolo wants to reduce model size and inference speed while keeping accuracy high.</p>

<h2 id="bag-of-freebies-only-cost-training-time">Bag of freebies (only cost training time)</h2>
<p>Augmentation, solving dataset bias (imbalance), better loss</p>

<h2 id="bag-of-specials-small-inference-cost-significantly-improvement-in-accuracy">Bag of specials (small inference cost, significantly improvement in accuracy)</h2>
<p>Enhance receptive field, attention - channelwise (squeeze-and-excitiation (SE)) - pointwise 
(Spatial Attention Module (SAM))
-&gt; SE ist costly on gpu
-&gt; SAM makes improvement and is efficient on gpu
Multi-scale prediction methods, Activationfunctions, Post-processing methods like nms (not needed in anchor-free
(without x1, x2, y1, y2) methods)</p>

<h1 id="color-quantization-using-modified-median-cut"><a href="http://leptonica.org/papers/mediancut.pdf">Color quantization using modified median cut</a></h1>

<h1 id="coherent-semantic-attention-for-image-inpainting">Coherent Semantic Attention for Image Inpainting</h1>
<p><a href="https://arxiv.org/pdf/1905.12384v3.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1905.12384v3-B3181B.svg" alt="Paper" /></a></p>

<h1 id="parallel-multiscale-autoregressive-density-estimation">Parallel Multiscale Autoregressive Density Estimation</h1>
<p><a href="https://arxiv.org/pdf/1703.03664.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1703.03664-B3181B.svg" alt="Paper" /></a></p>

<h1 id="eidetic3d-lstma-model-forvideoprediction-andbeyond"><a href="https://openreview.net/pdf?id=B1lKS2AqtX">EIDETIC3D LSTM:A MODEL FORVIDEOPREDICTION ANDBEYOND</a></h1>

<p>For perceiving and memorizin both short-term and long-term representations in videos.
This paper uses RNNs and 3D-CNNs as part of their architecture. These are both mechanisms for modeling spatio (image) temporal
data.
They found out that they have to somehow integrate the 3D-Convs inside the LSTM</p>

<h1 id="heated-upsoftmaxembedding">HEATED-UPSOFTMAXEMBEDDING</h1>
<p><a href="https://arxiv.org/pdf/1809.04157.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1809.04157-B3181B.svg" alt="Paper" /></a></p>

<h1 id="garmentgan-photo-realistic-adversarial-fashion-transfer">GarmentGAN: Photo-realistic Adversarial Fashion Transfer</h1>
<p><a href="https://arxiv.org/pdf/2003.01894v1.pdf"><img src="http://img.shields.io/badge/paper-arXiv.2003.01894v1-B3181B.svg" alt="Paper" /></a></p>

<p>Main Idea:
Cutting out the to be seperated part, transfrom the new garment onto that part.</p>

<p>Directly seen shortcomings:
A dress will be shortened onto a t-shirt part.</p>

<p>Ideas to be better:
Keep the aspect ratio and place the garment over other parts too.</p>

<p>The Problem of normal GANs is still blurred images and unrealistic</p>

<p>Garment transfer can be broken down into two components. First seperate the human body(pose, shape, color) from 
his clothes
then transfer the garment onto that body. This is done in a segmentation map space. So we first try to get
a realistic segmentatin map from the person in arbitrary pose wearing the desired cloth. This
phase is coarse.
As the first step they train a shape transfer network to produce a segmantic map, given an already segmented but masked map,
which is produced by a foreign masking network. 
They want the network to learn only to segment arms, upper torso and top clothes regions, therefore mask these regions. To
retain the hands, they use keypoints from a human body pose network. The create a line from the elbow to the wrist and append
a box at the end of the line over the hands, so that the side of the box is perpendicular to the line. All pixels, belonging
to the hand region and inside the box are retained. The Segmentation network does not have to learn them and complext
poses and gestures of the hand are retained. Everything that the network is not supposed to predict (everything not
masked) is overwritten by the input segmentation map. The identity of the person is preserved.</p>

<p>The Second stage comprises the transformation of the garment</p>

<h1 id="semantic-image-synthesis-with-spatially-adaptive-normalization">Semantic Image Synthesis with Spatially-Adaptive Normalization</h1>

<ul>
  <li>Where the SPADE Layer comes from. Its a Layer for creating an image from a segmentation map.</li>
</ul>

<p>Semantic image synthesis is about creating a photorealistic image from a segmentation map.</p>

<p>The Problem:</p>

<p>The Solution</p>

:ET