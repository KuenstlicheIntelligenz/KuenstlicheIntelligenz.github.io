I"a<p><strong>Work in progress</strong></p>

<p>I’ll try to give a summary over the components of Delf and Delg</p>

<h1 id="paper">Paper</h1>
<p>DELF:
<a href="https://arxiv.org/abs/1612.06321"><img src="http://img.shields.io/badge/paper-arXiv.1612.06321-B3181B.svg" alt="Paper" /></a></p>

<p>DELG:
<a href="https://arxiv.org/abs/2001.05027"><img src="http://img.shields.io/badge/paper-arXiv.2001.05027-B3181B.svg" alt="Paper" /></a></p>

<h1 id="gem">GeM</h1>
<p><a href="https://arxiv.org/pdf/1711.02512"><img src="http://img.shields.io/badge/paper-arXiv.1711.02512-B3181B.svg" alt="Paper" /></a></p>

<p>Generalized-mean (GeM) pooling.</p>

<h1 id="delg">Delg</h1>
<hr />

<p>In Delg we use <strong>local</strong> and <strong>global features</strong>. The local features are build from the shallow part of the CNN, for
example we use the output of <em>conv4</em> from ResNet50 and the output of <em>conv5</em> for the deeper global features.</p>

<h2 id="local-features">Local features</h2>
<p>Let 
<script type="math/tex">\chi_k</script> 
be the set of <script type="math/tex">W \times H</script> activations for feature map <script type="math/tex">k \in{\{1...K\}}</script> . Each point on a feature map has a receptive field 
on the original image and if its pixel is activated, its indicating the presence of that feature in that receptive field. We use
all channels from a pixel on a feature map as a local descriptor/feature. 
<img src="/assets/images/cnn_for_delf_desc.png" alt="" /></p>

<p>To reduce the size of the local feature vector, an autoencoder gets employed in Delg.</p>

<h2 id="global-feature">Global feature</h2>

<p>For the global feature vector we employ <strong>Generalized-Mean (GeM) pooling</strong> on the deeper output of the CNN with the aim
to reduce each feature map to one single number (global pooling).
To understand this ill quickly remind you of the pooling methods.</p>

<h3 id="global-max-pooling">Global Max Pooling</h3>

<p>There is <strong>global max pooling (MAC)</strong> given by:
\begin{equation}
f^{m}=[f_1^{m} … f_k^{m} .. f_K^{m}]^T, f_k^{m} = \max_{x \in \chi_k} x
\end{equation}
Max pooling on a feature map results in a number, indicating how present the feature is anywhere on the featuremap. If you
keep the location of the max you can tell where the feature is present on the image by the receptive field.
We implicitly compare one image patch.</p>

<h3 id="global-mean-pooling">Global Mean Pooling</h3>

<p>There is <strong>average pooling (SPoC)</strong> given by:
\begin{equation}
f^{a} = [ f_1^{a} … f_k^{a} … f_K^{a}]^T, f_k^{a}= \frac{1}{|\chi_k|} \sum_{x \in \chi_k} x
\end{equation}
Mean pooling describes how often and how strong a feature is present in the image overall.</p>

<h3 id="generalized-mean-pooling">Generalized-Mean Pooling</h3>

<p><strong>GeM</strong> pooling is a parametrized mixture of max and average pooling and given by:
\begin{equation}
f^{g}=[f_1^{g}…f_k^{g}…f_K^{g}]^T, f_k^{g} = ( \frac{1}{|\chi_k|} \sum_{x \in \chi_k} x^{p_k})^{ \frac{1}{p_k}}
\end{equation}
where the pooling parameter <script type="math/tex">p_k</script> can be manually set or learned. A large <script type="math/tex">p</script> would amplify single entries and
the operation would resemble max pooling. <br />
GeM pooling shows <strong>better results</strong> then mean or max pooling.</p>

<hr />

<p>One last question i had, what if we use mean and max pooling and let a fc decide how to use it? -&gt; No exponentiation, could
be faster and the CNN could optimize it…</p>

<h1 id="whitening-of-the-aggregated-representation-vector">Whitening of the aggregated representation vector</h1>
<p>Basically PCA + normalize the PCA Data.
Results in that all dimensions get equal contributions. Noise gets attenuated!
Whitening is only done on the train set! The validation data gets preprocessed with
the from training inferred parameters.</p>

<h1 id="arcface-margin">Arcface margin</h1>
<dl>
  <dt>Intra-class</dt>
  <dd>variation of objects in the same class</dd>
  <dt>Inter-class</dt>
  <dd>variation between objects of different classes</dd>
</dl>

<p>Softmax-cross-entropy loss does not optimize for high intra-class similarity of the feature vector and it 
does not optimize for diversity for inter-class samples.
We want something that puts together objects of the same class and put a large boundary between objects of different classes.</p>

<p>ArcFace is easy to implement, has low computational overhad and good convergence properties.
Consider the last two layers i and j of the CNN (Dense layers). <br />
Layer i has <script type="math/tex">d = 512</script> outputs, subsequent layer j has <script type="math/tex">n</script> output units
So we have <script type="math/tex">W \in R^{d \times n}</script> weights to compute
the output of the last layer. <script type="math/tex">x_i</script> is the output of the first layer.
Instead of directly computing the logits as <script type="math/tex">W_j^T x_i</script> we use the formula 
\begin{equation}
W_j^T x_i = |W_j| |x_i| \cos{\theta_j}
\end{equation} to get <script type="math/tex">cos(\theta)</script>. So we need to caluclate <script type="math/tex">\frac{W_j}{|W_j|}</script> and
<script type="math/tex">\frac{x_i}{|x_i|}</script>. Then we take the <script type="math/tex">\arccos</script> to get <script type="math/tex">\theta</script>. Here we add the additative 
margin penalty m. There are also SphereFace which multiplacative angular margin and CosFace with additive cosine
margin. 
Ganz hab ich noch nicht verstanden. Man nutzt arcface nur wenn in bestimmten winkeln.</p>

<p>from the last layer in the network,
we normalize the weights and the feature beforehand</p>

<h1 id="softplus-activation">Softplus activation</h1>
<p>Softplus is always positive and ranges from 0 to infinity. Its a smooth function and its differntiable at x = 0.
ReLU is more efficient to calculate</p>

<h1 id="qa">Q&amp;A</h1>

<p>What is the required input size for images?</p>
<ul>
  <li>You have to distinguish between training and inference. The Model is build fully convolutional and can handle arbitrary input sizes.
But for training we have to apply dense layers which define the input size.
At inference time we construct an image pyramid, extract features for each size and then select a subset or all features.
In Delf we trained the attention network seperately, delg has unified it. 
But i guess, the aspect ratio augmentation is now very important?</li>
</ul>

:ET