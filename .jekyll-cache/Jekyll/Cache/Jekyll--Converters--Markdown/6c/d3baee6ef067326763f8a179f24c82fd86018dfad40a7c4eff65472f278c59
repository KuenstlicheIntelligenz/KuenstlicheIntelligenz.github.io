I"√<h1 id="draft">Draft</h1>

<p><a href="https://arxiv.org/pdf/1911.10807v1.pdf">Fast and Generalized Adaptation for Few-Shot Learning</a></p>

<p>Few-Shot learning aimes to generalize to novel categories fast and with few training examples.
Problems of conventional classifiers are weak generalization and overfitting.</p>

<p>There are two approaches to solving this problem:</p>
<ol>
  <li>Metric based, where the classifiers embedded space (e.g. the output of a deeper layer) is used
to find anchors for classes. To classify an example we look at the nearest anchor.
    <ul>
      <li>Normaly poor generalization</li>
      <li>If retrained, prone to overfitting</li>
      <li>Discriminative features</li>
    </ul>
  </li>
  <li>Meta learning based, where a model is trained in a way that makes it quickly adapt to new
training task through fine-tuning. This is done by finding a versatile parameter space where the parameters ideally lie
in the center of needed changes for new tasks (MAML Based). Therefore the amount of change to generalize to new examples would be the
same for every new tasks, only the direction in the parameter space changes. Ideal would be only one gradient step. Another approach is
to find a subset of parameters that needs to be updated. (MTL)
    <ul>
      <li>Found parameter space can be biased and therefore the generalization ability suffers. (They not lie in the center,
therefore we would need e.g. one gradient step for task a but two for task b. If we do only one step, class a would be recognized
while task b suffers accuracy.)</li>
    </ul>
  </li>
</ol>

<p>The paper presents a two stage solution:</p>
<ol>
  <li>Prepare: Train a metric based classifier to attain discriminative features, the <strong>Adaptable Cosine Classifier</strong>. 
Also use the proposed meta learning method <strong>Amphibian</strong>.</li>
  <li>Adapt to new examples.</li>
</ol>

<p>Use mean-vectors of labled exmaples to parmetrize the cosine classifier?!
No retraining of the classifier to avoid overfitting.</p>

<p>The paper differentiates between the feature extractor and the classifier. The classifier is usually the last layer of the Neural Network
and a fully connected layer. Each class gets a set of weights which combine the extracted features to build the class probability. 
This is expressed through the dot product: <script type="math/tex">s_k = z^Tw_k^*</script>, where s_k is the raw classification score and k elem [1,K^<em>] the categories to
classify. w_k is the weight vector for class k. * Means everything combined. To get the probability of
class k, we have to use the *softmax</em> operator.<br />
If we add a new class, we add a new weight vector. This vector will predicted and differently learned then the base categories,
because we have to learn it faster.<br />
The problem is, that the raw classification scores can differ in their magnitude between the base classes and the newly learned class.
Therefore its not that easy to combine the classification for new and base classes. To deal with that problem, the paper introduces
the cosine similarity operator, which computes the raw classification score instead of the dot product.</p>

<ul>
  <li>The cosine classifier preserves the class neighborhood structure in the embedding space. Means that classes which are neighboors will have a lower distance then none neighbors. TODO: why is that so?</li>
</ul>

<p>!Though: This is just scaling the weights and fitting them to the mean</p>

<h1 id="cosine-similarity-coefficient">Cosine similarity coefficient</h1>

<p>So every
feature has one weight per class. A new class means that every feature gets a new weight.</p>

<ul>
  <li>
    <p>Leveraging an intermediate level of representation:<br />
___
  Usually means to cut of the network at some layer and get the features</p>
  </li>
  <li>
    <p>Semantic information about the categories to classify:<br />
  Descriptive information about the category e.g. as an numeric vector. The values of the vector describe an image.</p>
  </li>
</ul>

<p>Kann ich mal testen wie das hier funktioniert. Wir r√ºcke ich denn ein?</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Just with tab ```python import os test = 1 ```
</code></pre></div></div>
:ET