I"Y&<p><strong>Work in progress</strong></p>

<p>&nbsp;</p>

<h2 id="in-defense-of-the-triplet-loss-again-learning-robust-person-re-identificationwith-fast-approximated-triplet-loss-and-label-distillation">In Defense of the Triplet Loss Again: Learning Robust Person Re-Identificationwith Fast Approximated Triplet Loss and Label Distillation</h2>

<p><a href="https://arxiv.org/pdf/1912.07863.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1912.07863-B3181B.svg" alt="Paper" /></a></p>

<hr />

<h2 id="in-defense-of-the-triplet-loss-for-person-re-identification">In Defense of the Triplet Loss for Person Re-Identification</h2>

<p><a href="https://arxiv.org/pdf/1703.07737.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1703.07737-B3181B.svg" alt="Paper" /></a></p>

<hr />

<h2 id="combination-of-multiple-global-descriptors-for-image-retrieval">Combination of Multiple Global Descriptors for Image Retrieval</h2>

<p><a href="https://arxiv.org/pdf/1903.10663.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1903.10663-B3181B.svg" alt="Paper" /></a></p>

<p>Image similarity. Use SPoc Mac and GeM for pooling features and combines them.</p>

<p><a href="https://github.com/PuchatekwSzortach/combination_of_multiple_global_descriptors_for_image_retrieval">Code</a> with additional ideas (Keras Tensorflow)</p>

<hr />

<h2 id="high-performance-large-scale-image-recognition-without-normalization">High-Performance Large-Scale Image Recognition Without Normalization</h2>

<p><a href="https://arxiv.org/pdf/2102.06171v1.pdf"><img src="http://img.shields.io/badge/paper-arXiv.2102.06171v1-B3181B.svg" alt="Paper" /></a></p>

<p>Getting rid of Batchnormalization.
Better results then efficientnet while beeing faster.</p>

<hr />

<h2 id="softpool-refining-activation-downsampling-with-softpool">SoftPool: Refining activation downsampling with SoftPool</h2>

<p><a href="https://arxiv.org/pdf/2101.00440v2.pdf"><img src="http://img.shields.io/badge/paper-arXiv.2101.00440v2-B3181B.svg" alt="Paper" /></a></p>

<p>“Better” pooling (instead of Max, Mean, GeM)</p>

<hr />

<h2 id="sam-sharpness-aware-minimization-for-efficiently-improvinggeneralization">SAM: Sharpness-Aware Minimization for Efficiently ImprovingGeneralization</h2>

<p><a href="https://arxiv.org/pdf/2010.01412v2.pdf"><img src="http://img.shields.io/badge/paper-arXiv.2010.01412v2-B3181B.svg" alt="Paper" /></a></p>

<p>Optimize the network for a flat (unsharp) minima. Improves generalization and robustness to label noise.</p>

<hr />

<h2 id="delg-unifying-deep-local-and-global-features-for-image-search">DELG: Unifying Deep Local and Global Features for Image Search</h2>

<p><a href="https://arxiv.org/abs/2001.05027"><img src="http://img.shields.io/badge/paper-arXiv.2001.05027-B3181B.svg" alt="Paper" /></a></p>

<p>Image similarity and retrieval. Uses an image pyramid and GeM.</p>

<hr />

<h2 id="delf-large-scale-image-retrieval-with-attentive-deep-local-features">DELF: Large-Scale Image Retrieval with Attentive Deep Local Features</h2>

<p><a href="https://arxiv.org/abs/1612.06321"><img src="http://img.shields.io/badge/paper-arXiv.1612.06321-B3181B.svg" alt="Paper" /></a></p>

<p>Predecessor of DELG 
Image similarity based on learned local feature extraction</p>

<hr />

<h1 id="gem">GeM</h1>
<p><a href="https://arxiv.org/pdf/1711.02512"><img src="http://img.shields.io/badge/paper-arXiv.1711.02512-B3181B.svg" alt="Paper" /></a></p>

<p>Generalized-mean (GeM) pooling.</p>

<h1 id="mixed-pooling">Mixed Pooling</h1>
<p><a href="https://arxiv.org/pdf/1509.06033"><img src="http://img.shields.io/badge/paper-arXiv.1509.06033-B3181B.svg" alt="Paper" /></a></p>

<h1 id="comparison-of-mean-max-and-hybridboth-pooling">Comparison of mean, max and hybrid(both) pooling</h1>
<p><a href="https://arxiv.org/pdf/1509.06033.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1509.06033-B3181B.svg" alt="Paper" /></a>
Illustration of semantic information captured by each feature map of pool5 layer using CNN
-&gt; Mean pooling is better then just concat mean and max (hybrid)</p>

<h1 id="retrieving-similar-e-commerce-images-using-deep-learning">Retrieving Similar E-Commerce Images Using Deep Learning</h1>

<ul>
  <li>
    <p>Siamese architecture (2 images?), angular loss, combination of low and toplevel,
fractional distance matrix to calc distance between features</p>
  </li>
  <li>
    <p>Manhatten distance matrix provides the best discrimination in high dimensional data spaces.
L_k norm with k smaller then 1</p>
  </li>
</ul>

<h1 id="graph-neural-networksa-review-of-methods-and-applications">Graph Neural Networks:A Review of Methods and Applications</h1>
<p><a href="https://arxiv.org/pdf/1812.08434.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1812.08434-B3181B.svg" alt="Paper" /></a></p>

<h1 id="self-correction-for-human-parsing">Self-Correction for Human Parsing</h1>
<p><a href="https://arxiv.org/pdf/1910.09777v1.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1910.09777-B3181B.svg" alt="Paper" /></a></p>

<h1 id="yolov4-optimal-speed-and-accuracy-of-object-detection">YOLOv4: Optimal Speed and Accuracy of Object Detection</h1>
<p><a href="https://arxiv.org/pdf/2004.10934v1.pdf"><img src="http://img.shields.io/badge/paper-arXiv.2004.10934v1-B3181B.svg" alt="Paper" /></a></p>

<p>Yolo wants to reduce model size and inference speed while keeping accuracy high.</p>

<h2 id="bag-of-freebies-only-cost-training-time">Bag of freebies (only cost training time)</h2>
<p>Augmentation, solving dataset bias (imbalance), better loss</p>

<h2 id="bag-of-specials-small-inference-cost-significantly-improvement-in-accuracy">Bag of specials (small inference cost, significantly improvement in accuracy)</h2>
<p>Enhance receptive field, attention - channelwise (squeeze-and-excitiation (SE)) - pointwise 
(Spatial Attention Module (SAM))
-&gt; SE ist costly on gpu
-&gt; SAM makes improvement and is efficient on gpu
Multi-scale prediction methods, Activationfunctions, Post-processing methods like nms (not needed in anchor-free
(without x1, x2, y1, y2) methods)</p>

<h1 id="color-quantization-using-modified-median-cut"><a href="http://leptonica.org/papers/mediancut.pdf">Color quantization using modified median cut</a></h1>

<h1 id="coherent-semantic-attention-for-image-inpainting">Coherent Semantic Attention for Image Inpainting</h1>
<p><a href="https://arxiv.org/pdf/1905.12384v3.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1905.12384v3-B3181B.svg" alt="Paper" /></a></p>

<h1 id="parallel-multiscale-autoregressive-density-estimation">Parallel Multiscale Autoregressive Density Estimation</h1>
<p><a href="https://arxiv.org/pdf/1703.03664.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1703.03664-B3181B.svg" alt="Paper" /></a></p>

<h1 id="eidetic3d-lstma-model-forvideoprediction-andbeyond"><a href="https://openreview.net/pdf?id=B1lKS2AqtX">EIDETIC3D LSTM:A MODEL FORVIDEOPREDICTION ANDBEYOND</a></h1>

<p>For perceiving and memorizin both short-term and long-term representations in videos.
This paper uses RNNs and 3D-CNNs as part of their architecture. These are both mechanisms for modeling spatio (image) temporal
data.
They found out that they have to somehow integrate the 3D-Convs inside the LSTM</p>

<h1 id="heated-upsoftmaxembedding">HEATED-UPSOFTMAXEMBEDDING</h1>
<p><a href="https://arxiv.org/pdf/1809.04157.pdf"><img src="http://img.shields.io/badge/paper-arXiv.1809.04157-B3181B.svg" alt="Paper" /></a></p>

<h1 id="garmentgan-photo-realistic-adversarial-fashion-transfer">GarmentGAN: Photo-realistic Adversarial Fashion Transfer</h1>
<p><a href="https://arxiv.org/pdf/2003.01894v1.pdf"><img src="http://img.shields.io/badge/paper-arXiv.2003.01894v1-B3181B.svg" alt="Paper" /></a></p>

<p>Main Idea:
Cutting out the to be seperated part, transfrom the new garment onto that part.</p>

<p>Directly seen shortcomings:
A dress will be shortened onto a t-shirt part.</p>

<p>Ideas to be better:
Keep the aspect ratio and place the garment over other parts too.</p>

<p>The Problem of normal GANs is still blurred images and unrealistic</p>

<p>Garment transfer can be broken down into two components. First seperate the human body(pose, shape, color) from 
his clothes
then transfer the garment onto that body. This is done in a segmentation map space. So we first try to get
a realistic segmentatin map from the person in arbitrary pose wearing the desired cloth. This
phase is coarse.
As the first step they train a shape transfer network to produce a segmantic map, given an already segmented but masked map,
which is produced by a foreign masking network. 
They want the network to learn only to segment arms, upper torso and top clothes regions, therefore mask these regions. To
retain the hands, they use keypoints from a human body pose network. The create a line from the elbow to the wrist and append
a box at the end of the line over the hands, so that the side of the box is perpendicular to the line. All pixels, belonging
to the hand region and inside the box are retained. The Segmentation network does not have to learn them and complext
poses and gestures of the hand are retained. Everything that the network is not supposed to predict (everything not
masked) is overwritten by the input segmentation map. The identity of the person is preserved.</p>

<p>The Second stage comprises the transformation of the garment</p>

<h1 id="semantic-image-synthesis-with-spatially-adaptive-normalization">Semantic Image Synthesis with Spatially-Adaptive Normalization</h1>

<ul>
  <li>Where the SPADE Layer comes from. Its a Layer for creating an image from a segmentation map.</li>
</ul>

<p>Semantic image synthesis is about creating a photorealistic image from a segmentation map.</p>

<p>The Problem:</p>

<p>The Solution</p>

:ET