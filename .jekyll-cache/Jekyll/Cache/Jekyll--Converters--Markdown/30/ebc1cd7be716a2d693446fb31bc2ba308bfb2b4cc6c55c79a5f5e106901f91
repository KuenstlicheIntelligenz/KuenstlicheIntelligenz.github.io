I"À<p><strong>Work in progress</strong></p>

<p>Iâ€™ll try to give a summary over the components of Delf and Delg</p>

<h1 id="paper">Paper</h1>
<p>DELF:
<a href="https://arxiv.org/abs/1612.06321"><img src="http://img.shields.io/badge/paper-arXiv.1612.06321-B3181B.svg" alt="Paper" /></a></p>

<p>DELG:
<a href="https://arxiv.org/abs/2001.05027"><img src="http://img.shields.io/badge/paper-arXiv.2001.05027-B3181B.svg" alt="Paper" /></a></p>

<h1 id="gem">GeM</h1>
<p><a href="https://arxiv.org/pdf/1711.02512"><img src="http://img.shields.io/badge/paper-arXiv.1711.02512-B3181B.svg" alt="Paper" /></a></p>

<p>Generalized-mean (GeM) pooling.</p>

<p>Let 
<script type="math/tex">\chi_k</script> 
be the set of <script type="math/tex">W \times H</script> activations for feature map <script type="math/tex">k \in{\{1...K\}}</script> . These are the feature maps
coming out of an Neural network. Each point on a feature map has a receptive field on the original image and if
its pixel is activated, its denoting a presence of that feature in that receptive field.</p>

<p>There is <strong>global max pooling (MAC)</strong> given by:
\begin{equation}
f^{m}=[f_1^{m} â€¦ f_k^{m} .. f_K^{m}]^T, f_k^{m} = \max_{x \in \chi_k} x
\end{equation}
It creates a vector which denotes if a feature is present anywhere on the feature map. If you keep the location of the max in the
feature map, you can see that the resulting descriptorcomponent corresponds to an image patch equal to the receptive field. Or in 
other words, each component of the descriptor marks an image patch. So with MAC we implicitly compare image patches.</p>

<p>There is <strong>average pooling (SPoC)</strong> given by:
\begin{equation}
f^{a} = [ f_1^{a} â€¦ f_k^{a} â€¦ f_K^{a}]^T, f_k^{a}= \frac{1}{|\chi_k|} \sum_{x \in \chi_k} x
\end{equation}
Mean pooling describes how often and how strong a feature is present in the image overall.</p>

<p><strong>GeM</strong> pooling is a parametrized mixture of max and average pooling and given by:
\begin{equation}
f^{g}=[f_1^{g}â€¦f_k^{g}â€¦f_K^{g}]^T, f_k^{g} = ( \frac{1}{|\chi_k|} \sum_{x \in \chi_k} x^{p_k})^{ \frac{1}{p_k}}
\end{equation}
where the pooling parameter <script type="math/tex">p_k</script> can be manually set or learned. The larger the <script type="math/tex">p</script> the more the 
operation resembles max pooling.</p>
<blockquote>
  <p>It weights the contributions of each feature.</p>
</blockquote>

<p>GeM pooling shows better results.</p>

<h1 id="whitening-of-the-aggregated-representation-vector">Whitening of the aggregated representation vector</h1>
<p>Basically PCA + normalize the PCA Data.
Results in that all dimensions get equal contributions. Noise gets attenuated!
Whitening is only done on the train set! The validation data gets preprocessed with
the from training inferred parameters.</p>

<h1 id="what-means-intra-class-and-inter-class">What means intra-class and inter-class</h1>
<p>Intra-class means the variation of objects in the same class.
Inter-class means the variation between objects of different classes</p>

<h1 id="arcface-margin">arcface margin</h1>
<p>Softmax-cross-entropy loss does not optimize for high intra-class similarity of the feature vector and it 
does not optimize for diversity for inter-class samples.
ArcFace is easy to implement, has low computational overhad and good convergence properties.
It is about the last 2 layers in the DNN, layer i and following j. For example we have layer i 
with <script type="math/tex">d = 512</script> outputs.
and subsequent layer with <script type="math/tex">n</script> output units. So we have <script type="math/tex">W \in R^{d \times n}</script> weights to compute
the output of the last layer. <script type="math/tex">x_i</script> is the output of the first layer.
Instead of directly computing the logits as <script type="math/tex">W_j^T x_i</script> we use the formula 
\begin{equation}
W_j^T x_i = |W_j| |x_i| \cos{\theta_j}
\end{equation} to get <script type="math/tex">cos(\theta)</script>. So we need to caluclate <script type="math/tex">\frac{W_j}{|W_j|}</script> and
<script type="math/tex">\frac{x_i}{|x_i|}</script>. Then we take the <script type="math/tex">\arccos</script> to get <script type="math/tex">\theta</script>. Here we add the additative 
margin penalty m. There are also SphereFace which multiplacative angular margin and CosFace with additive cosine
margin. 
Ganz hab ich noch nicht verstanden. Man nutzt arcface nur wenn in bestimmten winkeln.</p>

<p>from the last layer in the network,
we normalize the weights and the feature beforehand</p>

<h1 id="softplus-activation">Softplus activation</h1>
<p>Softplus is always positive and ranges from 0 to infinity. Its a smooth function and its differntiable at x = 0.
ReLU is more efficient to calculate</p>

:ET